# -*- coding: utf-8 -*-
"""modelling

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AbWPW8Thlb49vMLxmqYCU16PwFtIwRZW
"""

import os
import numpy as np
import pandas as pd
DATA_PATH = "titanic_preprocessing.csv"
TARGET = "Survived"
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"File tidak ketemu: {DATA_PATH}. Upload/clone dulu ke Colab.")
df = pd.read_csv(DATA_PATH)
print("Shape:", df.shape)
print("Columns:", df.columns.tolist())
if TARGET not in df.columns:
    raise ValueError(f"Target '{TARGET}' tidak ada. Kolom ada: {df.columns.tolist()}")

X = df.drop(columns=[TARGET]).copy()
y = df[TARGET].copy()

"""split data"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y if y.nunique() == 2 else None)
print("Train:", X_train.shape, "Test:", X_test.shape)

"""preproc+model pipeline"""

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression

num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()
numeric_tf = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler()),
])
categorical_tf = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("ohe", OneHotEncoder(handle_unknown="ignore")),
])
preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_tf, num_cols),
        ("cat", categorical_tf, cat_cols),
    ],
    remainder="drop")
model = LogisticRegression(
    max_iter=2000,
    solver="liblinear",
    class_weight="balanced")

pipe = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("model", model)
])
pipe

"""train&evaluate"""

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report)
pipe.fit(X_train, y_train)

y_pred = pipe.predict(X_test)
try:
    y_proba = pipe.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_proba)
except Exception:
    y_proba, auc = None, None

acc  = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred, zero_division=0)
rec  = recall_score(y_test, y_pred, zero_division=0)
f1   = f1_score(y_test, y_pred, zero_division=0)
print("Accuracy :", acc)
print("Precision:", prec)
print("Recall   :", rec)
print("F1       :", f1)
print("ROC AUC  :", auc)
print("\nClassification report:\n", classification_report(y_test, y_pred, digits=4, zero_division=0))

"""simpan artifak"""

import json
import matplotlib.pyplot as plt
from datetime import datetime

OUT_DIR = "artifacts/extras"
os.makedirs(OUT_DIR, exist_ok=True)

cm = confusion_matrix(y_test, y_pred)

#cm png
plt.figure()
plt.imshow(cm)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.xticks([0, 1], ["0", "1"])
plt.yticks([0, 1], ["0", "1"])
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, str(cm[i, j]), ha="center", va="center")
plt.tight_layout()
cm_path = os.path.join(OUT_DIR, "confusion_matrix.png")
plt.savefig(cm_path, dpi=150)
plt.close()
# classification report txt
rep_path = os.path.join(OUT_DIR, "classification_report.txt")
with open(rep_path, "w", encoding="utf-8") as f:
    f.write(classification_report(y_test, y_pred, digits=4, zero_division=0))
# metadata json
meta_path = os.path.join(OUT_DIR, "run_metadata.json")
meta = {
    "timestamp": datetime.now().isoformat(),
    "target": TARGET,
    "features": list(X.columns),
    "dtypes": {c: str(df[c].dtype) for c in df.columns},
}
with open(meta_path, "w", encoding="utf-8") as f:
    json.dump(meta, f, indent=2)

print("Saved:", cm_path, rep_path, meta_path)

"""mlflow logging"""

import mlflow
import mlflow.sklearn
EXPERIMENT_NAME = "workflow_ci_titanic"
RUN_NAME = "colab_logreg_training"

mlflow.set_experiment(EXPERIMENT_NAME)
with mlflow.start_run(run_name=RUN_NAME):
    # params
    mlflow.log_param("data_path", DATA_PATH)
    mlflow.log_param("target", TARGET)
    mlflow.log_param("test_size", 0.2)
    mlflow.log_param("random_state", 42)
    mlflow.log_param("model", "LogisticRegression")
    mlflow.log_param("class_weight", "balanced")
    mlflow.log_param("num_features", len(X.columns))
    mlflow.log_param("num_numeric_cols", len(num_cols))
    mlflow.log_param("num_categorical_cols", len(cat_cols))
    # metrics
    mlflow.log_metric("test_accuracy", float(acc))
    mlflow.log_metric("test_precision", float(prec))
    mlflow.log_metric("test_recall", float(rec))
    mlflow.log_metric("test_f1", float(f1))
    if auc is not None:
        mlflow.log_metric("test_roc_auc", float(auc))
    # artifacts
    mlflow.log_artifacts("artifacts/extras", artifact_path="extras")
    # model
    mlflow.sklearn.log_model(pipe, artifact_path="model")

print("DONE: MLflow run created.")

"""dagshub"""

REPO_OWNER = "rafi1n"
REPO_NAME  = "Eksperimen_SML_Muhammad-Rafi-Insani"
from getpass import getpass
token = getpass("Paste DAGSHUB TOKEN (tidak tampil): ")
os.environ["DAGSHUB_TOKEN"] = token
print("DAGSHUB_TOKEN set.")

import dagshub
dagshub.init(repo_owner=REPO_OWNER, repo_name=REPO_NAME, mlflow=True)
print("MLflow tracking URI:", mlflow.get_tracking_uri())

EXPERIMENT_NAME = "titanic_colab_baseline_20251221"
mlflow.set_experiment(EXPERIMENT_NAME)
print("Experiment set:", EXPERIMENT_NAME)

import mlflow.sklearn

RUN_NAME = "colab_logreg_dagshub"

with mlflow.start_run(run_name=RUN_NAME):
    mlflow.log_param("repo_owner", REPO_OWNER)
    mlflow.log_param("repo_name", REPO_NAME)

    mlflow.log_metric("test_accuracy", float(acc))
    mlflow.log_metric("test_precision", float(prec))
    mlflow.log_metric("test_recall", float(rec))
    mlflow.log_metric("test_f1", float(f1))
    if auc is not None:
        mlflow.log_metric("test_roc_auc", float(auc))

    # upload artifacts folder
    mlflow.log_artifacts("artifacts/extras", artifact_path="extras")
    # log model
    mlflow.sklearn.log_model(pipe, artifact_path="model")

print("DONE: logged to DagsHub.")